A framework for evaluating Retrieval-Augmented Generation (RAG) models and pipelines.

Overview
This repository aims to provide tools and Jupyter notebooks to systematically evaluate the effectiveness and accuracy of RAG-based systems. RAG combines information retrieval with generative language models to answer questions, summarize documents, or generate context-aware responses.

This project is designed for researchers, engineers, and practitioners who want to:

Test and benchmark different RAG architectures

Analyze retrieval and generation performance

Customize the evaluation pipeline for various datasets

Features
Evaluation scripts and Jupyter Notebooks for:

Measuring retrieval relevance and accuracy

Assessing generative quality (e.g., factual correctness, fluency)

Analyzing end-to-end performance metrics

Modular structure for integrating custom retrieval and generation components

Open to contributions and extensions (additional metrics, evaluation datasets, model adapters)
