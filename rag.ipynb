{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f55a4bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install datasets ir_datasets rank_bm25 sentence_transformers keras langchain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3e4fadc",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74420361",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from ir_datasets import load as load_ir\n",
    "from rank_bm25 import BM25Okapi\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbbc5942",
   "metadata": {},
   "source": [
    "##  Download data from https://huggingface.co/datasets/mandarjoshi/trivia_qa/tree/main/unfiltered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cfc7231",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"parquet\", data_files=\"train-00000-of-00047.parquet\", split=\"train[:50]\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "380560e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['question', 'question_id', 'question_source', 'entity_pages', 'search_results', 'answer'])\n"
     ]
    }
   ],
   "source": [
    "print(ds[0].keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32feff40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['doc_source', 'filename', 'title', 'wiki_context'])\n"
     ]
    }
   ],
   "source": [
    "print(ds[0][\"entity_pages\"].keys())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a478588e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['description', 'filename', 'rank', 'title', 'url', 'search_context'])\n"
     ]
    }
   ],
   "source": [
    "print(ds[0][\"search_results\"].keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abc70035",
   "metadata": {},
   "source": [
    "# Prepare a dataset for document retrieval:\n",
    "# - Deduplicate documents and store them with optional metadata\n",
    "# - Build mappings of question index -> gold documents\n",
    "# - Build mappings of question index -> candidate documents (including golds)\n",
    "# - Compute basic stats: total unique docs, avg candidates and golds per question\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79b41992",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed questions: 50\n",
      "Unique documents in corpus: 528\n",
      "Avg candidates per question: 10.76\n",
      "Avg gold docs per question: 1.66\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "# Limit for teaching purposes (you can increase)\n",
    "N = len(ds)  # or smaller, e.g., 50\n",
    "\n",
    "doc_texts = []        # all unique docs for FAISS\n",
    "doc_meta  = []        # optional metadata\n",
    "doc_key_to_id = {}    # map text -> index for deduplication\n",
    "\n",
    "q_gold = {}       # question idx -> set(doc_id) (gold)\n",
    "q_candidates = {} # question idx -> list(doc_id) (candidate pool)\n",
    "\n",
    "def add_doc(text, meta=None):\n",
    "    t = text.strip()\n",
    "    if not t:\n",
    "        return None\n",
    "    if t in doc_key_to_id:\n",
    "        return doc_key_to_id[t]\n",
    "    doc_id = len(doc_texts)\n",
    "    doc_texts.append(t)\n",
    "    doc_meta.append(meta)\n",
    "    doc_key_to_id[t] = doc_id\n",
    "    return doc_id\n",
    "\n",
    "for i in range(N):\n",
    "    item = ds[i]\n",
    "    \n",
    "    # --- Gold docs ---\n",
    "    gold_ids = set()\n",
    "    for txt in item[\"entity_pages\"][\"wiki_context\"]:\n",
    "        did = add_doc(txt, meta={\"type\":\"gold\"})\n",
    "        if did is not None:\n",
    "            gold_ids.add(did)\n",
    "    \n",
    "    # --- Candidate docs ---\n",
    "    cand_ids = []\n",
    "    for txt in item[\"search_results\"][\"search_context\"]:\n",
    "        did = add_doc(txt, meta={\"type\":\"candidate\"})\n",
    "        if did is not None:\n",
    "            cand_ids.append(did)\n",
    "    \n",
    "    # Ensure gold docs are in candidate list\n",
    "    for gid in list(gold_ids):\n",
    "        if gid not in cand_ids:\n",
    "            cand_ids.append(gid)\n",
    "    \n",
    "    q_gold[i] = gold_ids\n",
    "    q_candidates[i] = cand_ids\n",
    "\n",
    "# Summary\n",
    "print(\"Processed questions:\", N)\n",
    "print(\"Unique documents in corpus:\", len(doc_texts))\n",
    "avg_cands = sum(len(v) for v in q_candidates.values()) / max(1, N)\n",
    "avg_golds = sum(len(v) for v in q_gold.values()) / max(1, N)\n",
    "print(f\"Avg candidates per question: {avg_cands:.2f}\")\n",
    "print(f\"Avg gold docs per question: {avg_golds:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "510bd5c8",
   "metadata": {},
   "source": [
    "# Build embeddings and FAISS index for candidate documents:\n",
    "# 1. Load a SentenceTransformer model for generating dense embeddings.\n",
    "# 2. Compute approximate token count and total documents for info.\n",
    "# 3. Encode all candidate documents into embeddings and measure time.\n",
    "# 4. Create a FAISS index (L2 distance) and add embeddings for fast similarity search.\n",
    "# 5. Print timing and summary stats.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "601827e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Approximate total tokens in candidate docs: 1532383\n",
      "Total candidate docs: 528\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01131a13d9a042c5b5214eac2bd05233",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/17 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to embed 528 docs (1532383 tokens approx): 51.51 seconds\n",
      "Time taken to add embeddings to FAISS index: 0.00 seconds\n",
      "FAISS index built with 528 documents\n"
     ]
    }
   ],
   "source": [
    "import faiss\n",
    "import numpy as np\n",
    "import time\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Step 2a: Build embeddings for all candidate docs\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Count approximate tokens (split by space)\n",
    "num_tokens = sum(len(doc.split()) for doc in doc_texts)\n",
    "print(\"Approximate total tokens in candidate docs:\", num_tokens)\n",
    "print(\"Total candidate docs:\", len(doc_texts))\n",
    "\n",
    "# Measure time for embedding\n",
    "start_time = time.time()\n",
    "cand_embeddings = model.encode(doc_texts, convert_to_numpy=True, show_progress_bar=True)\n",
    "end_time = time.time()\n",
    "print(f\"Time taken to embed {len(doc_texts)} docs ({num_tokens} tokens approx): {end_time - start_time:.2f} seconds\")\n",
    "\n",
    "# Step 2b: Build FAISS index\n",
    "start_index_time = time.time()\n",
    "embedding_dim = cand_embeddings.shape[1]\n",
    "index = faiss.IndexFlatL2(embedding_dim)  # L2 distance\n",
    "index.add(cand_embeddings)\n",
    "end_index_time = time.time()\n",
    "print(f\"Time taken to add embeddings to FAISS index: {end_index_time - start_index_time:.2f} seconds\")\n",
    "print(\"FAISS index built with\", index.ntotal, \"documents\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd61a0ba",
   "metadata": {},
   "source": [
    "# Retrieve and evaluate documents for a single question:\n",
    "# 1. Select a question and its gold document IDs.\n",
    "# 2. Embed the question using the same model as for candidates.\n",
    "# 3. Retrieve top-k most similar documents from the FAISS index.\n",
    "# 4. Display retrieved snippets and mark which are gold.\n",
    "# 5. Compute basic retrieval metrics: Precision@k, Recall@k, and Reciprocal Rank (RR).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e7418f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Who was President when the first Peanuts cartoon was published?\n",
      "Gold doc IDs: {0}\n",
      "\n",
      "Top-k retrieved doc IDs: [5 1 4 0 3]\n",
      "\n",
      "Rank 1 - Doc ID 5 ❌:\n",
      "A Brief History of Charles Schulz's 'Peanuts' Comic Strip - TIME Follow @TIME When Alex Davis was 2 years old, he pointed to a drawing his father had done and exclaimed, \"Snoopy!\" The problem: his father was Jim Davis, the creator of Garfield, and the picture was of the cat he made famous. Charles S ...\n",
      "\n",
      "Rank 2 - Doc ID 1 ❌:\n",
      "Peanuts | Peanuts Wiki | Fandom powered by Wikia Charles M. Schulz drawing Snoopy . Peanuts is a syndicated daily and Sunday comic strip written and illustrated by Charles M. Schulz , which ran from October 2, 1950, to February 13, 2000 (the day after Schulz's death). In total 17,897 different Peanu ...\n",
      "\n",
      "Rank 3 - Doc ID 4 ❌:\n",
      "Peanuts by Charles Schulz  | Read Comic Strips at GoComics.com Share Link Explore Peanuts ...\n",
      "\n",
      "Rank 4 - Doc ID 0 ✅ GOLD:\n",
      "Peanuts is a syndicated daily and Sunday American comic strip written and illustrated by Charles M. Schulz, which ran from October 2, 1950, to February 13, 2000, continuing in reruns afterward. The strip is the most popular and influential in the history of comic strips, with 17,897 strips published ...\n",
      "\n",
      "Rank 5 - Doc ID 3 ❌:\n",
      "Frequently Asked Questions - Charles M. Schulz Museum Charles M. Schulz Museum The Peanuts Comic Strip Why is the comic strip named Peanuts? Originally, Charles Schulz named his strip Li’l Folks, but when it became syndicated in 1950 by United Feature Syndicate, there was concern about possible copy ...\n",
      "\n",
      "Metrics for this question:\n",
      "Precision@5: 0.200\n",
      "Recall@5: 1.000\n",
      "Reciprocal Rank (RR): 0.250\n"
     ]
    }
   ],
   "source": [
    "# Pick one question\n",
    "i = 0\n",
    "question = ds[i][\"question\"]\n",
    "gold_ids = q_gold[i]  # set of gold doc IDs\n",
    "\n",
    "print(\"Question:\", question)\n",
    "print(\"Gold doc IDs:\", gold_ids)\n",
    "\n",
    "# Embed the question\n",
    "q_emb = model.encode(question, convert_to_numpy=True)\n",
    "\n",
    "# Retrieve top-k docs\n",
    "top_k = 5\n",
    "D, I = index.search(np.expand_dims(q_emb, axis=0), top_k)\n",
    "retrieved_ids = I[0]\n",
    "\n",
    "print(\"\\nTop-k retrieved doc IDs:\", retrieved_ids)\n",
    "\n",
    "# Show retrieved snippets and indicate gold\n",
    "for rank, doc_id in enumerate(retrieved_ids, start=1):\n",
    "    snippet = doc_texts[doc_id][:300].replace(\"\\n\", \" \")\n",
    "    is_gold = \"✅ GOLD\" if doc_id in gold_ids else \"❌\"\n",
    "    print(f\"\\nRank {rank} - Doc ID {doc_id} {is_gold}:\\n{snippet} ...\")\n",
    "\n",
    "# Compute metrics for this single question\n",
    "precision = len(gold_ids & set(retrieved_ids)) / top_k\n",
    "recall = len(gold_ids & set(retrieved_ids)) / max(1, len(gold_ids))\n",
    "rr = 0\n",
    "for rank, doc_id in enumerate(retrieved_ids):\n",
    "    if doc_id in gold_ids:\n",
    "        rr = 1 / (rank + 1)\n",
    "        break\n",
    "\n",
    "print(f\"\\nMetrics for this question:\")\n",
    "print(f\"Precision@{top_k}: {precision:.3f}\")\n",
    "print(f\"Recall@{top_k}: {recall:.3f}\")\n",
    "print(f\"Reciprocal Rank (RR): {rr:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0221ef67",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b22352cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4dd68f36",
   "metadata": {},
   "source": [
    "# Re-rank retrieved documents using a cross-encoder:\n",
    "# 1. Load a cross-encoder model fine-tuned for relevance/QA.\n",
    "# 2. Retrieve top-k candidate documents from FAISS using sentence embeddings.\n",
    "# 3. Create (question, document) pairs for cross-encoder scoring.\n",
    "# 4. Compute relevance scores and sort documents by descending score.\n",
    "# 5. Display reranked top-k snippets and indicate gold documents.\n",
    "# 6. Compute retrieval metrics (Precision@k, Recall@k, Reciprocal Rank) after reranking.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a66a658",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reranked top-k doc IDs: [1, 0, 5, 3, 4]\n",
      "\n",
      "Rank 1 - Doc ID 1 ❌:\n",
      "Peanuts | Peanuts Wiki | Fandom powered by Wikia Charles M. Schulz drawing Snoopy . Peanuts is a syndicated daily and Sunday comic strip written and illustrated by Charles M. Schulz , which ran from October 2, 1950, to February 13, 2000 (the day after Schulz's death). In total 17,897 different Peanu ...\n",
      "\n",
      "Rank 2 - Doc ID 0 ✅ GOLD:\n",
      "Peanuts is a syndicated daily and Sunday American comic strip written and illustrated by Charles M. Schulz, which ran from October 2, 1950, to February 13, 2000, continuing in reruns afterward. The strip is the most popular and influential in the history of comic strips, with 17,897 strips published ...\n",
      "\n",
      "Rank 3 - Doc ID 5 ❌:\n",
      "A Brief History of Charles Schulz's 'Peanuts' Comic Strip - TIME Follow @TIME When Alex Davis was 2 years old, he pointed to a drawing his father had done and exclaimed, \"Snoopy!\" The problem: his father was Jim Davis, the creator of Garfield, and the picture was of the cat he made famous. Charles S ...\n",
      "\n",
      "Rank 4 - Doc ID 3 ❌:\n",
      "Frequently Asked Questions - Charles M. Schulz Museum Charles M. Schulz Museum The Peanuts Comic Strip Why is the comic strip named Peanuts? Originally, Charles Schulz named his strip Li’l Folks, but when it became syndicated in 1950 by United Feature Syndicate, there was concern about possible copy ...\n",
      "\n",
      "Rank 5 - Doc ID 4 ❌:\n",
      "Peanuts by Charles Schulz  | Read Comic Strips at GoComics.com Share Link Explore Peanuts ...\n",
      "\n",
      "Metrics after reranking:\n",
      "Precision@5: 0.200\n",
      "Recall@5: 1.000\n",
      "Reciprocal Rank (RR): 0.500\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import CrossEncoder\n",
    "\n",
    "# Load cross-encoder (fine-tuned for relevance / QA)\n",
    "cross_model = CrossEncoder(\"cross-encoder/ms-marco-MiniLM-L-6-v2\")\n",
    "\n",
    "# Example: single question\n",
    "i = 0\n",
    "question = ds[i][\"question\"]\n",
    "gold_ids = q_gold[i]\n",
    "\n",
    "# Step 1: retrieve top-k using FAISS\n",
    "top_k = 5\n",
    "q_emb = model.encode(question, convert_to_numpy=True)  # use your sentence-transformer\n",
    "D, I = index.search(np.expand_dims(q_emb, axis=0), top_k)\n",
    "retrieved_ids = I[0]\n",
    "\n",
    "# Step 2: prepare pairs for cross-encoder scoring\n",
    "pairs = [[question, doc_texts[doc_id]] for doc_id in retrieved_ids]\n",
    "\n",
    "# Step 3: compute relevance scores\n",
    "scores = cross_model.predict(pairs)\n",
    "\n",
    "# Step 4: sort by score (descending)\n",
    "sorted_idx = np.argsort(scores)[::-1]\n",
    "reranked_ids = [retrieved_ids[idx] for idx in sorted_idx]\n",
    "\n",
    "# Step 5: display reranked docs and compute metrics\n",
    "print(\"Reranked top-k doc IDs:\", reranked_ids)\n",
    "for rank, doc_id in enumerate(reranked_ids, start=1):\n",
    "    snippet = doc_texts[doc_id][:300].replace(\"\\n\", \" \")\n",
    "    is_gold = \"✅ GOLD\" if doc_id in gold_ids else \"❌\"\n",
    "    print(f\"\\nRank {rank} - Doc ID {doc_id} {is_gold}:\\n{snippet} ...\")\n",
    "\n",
    "# Step 6: compute metrics after reranking\n",
    "precision = len(gold_ids & set(reranked_ids)) / top_k\n",
    "recall = len(gold_ids & set(reranked_ids)) / max(1, len(gold_ids))\n",
    "rr = 0\n",
    "for rank, doc_id in enumerate(reranked_ids):\n",
    "    if doc_id in gold_ids:\n",
    "        rr = 1 / (rank + 1)\n",
    "        break\n",
    "\n",
    "print(f\"\\nMetrics after reranking:\")\n",
    "print(f\"Precision@{top_k}: {precision:.3f}\")\n",
    "print(f\"Recall@{top_k}: {recall:.3f}\")\n",
    "print(f\"Reciprocal Rank (RR): {rr:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74f5d891",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97978651",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "45b5fc92",
   "metadata": {},
   "source": [
    "# Trial with CromaDB "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cad13cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install chromadb\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "\n",
    "# Initialize Chroma client with new configuration\n",
    "client = chromadb.Client(Settings())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b58ba8b",
   "metadata": {},
   "source": [
    "# Store candidate documents in a Chroma vector database:\n",
    "# 1. Create a Chroma collection (delete if it already exists).\n",
    "# 2. Prepare document IDs and optional metadata.\n",
    "# 3. Add documents and their embeddings to the collection.\n",
    "# 4. Print confirmation of collection creation and document count.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4543748",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chroma collection 'candidate_docs' created with 528 documents.\n"
     ]
    }
   ],
   "source": [
    "# Install Chroma if not already\n",
    "# pip install chromadb\n",
    "\n",
    "\n",
    "\n",
    "# Create a collection\n",
    "collection_name = \"candidate_docs\"\n",
    "if collection_name in [c.name for c in client.list_collections()]:\n",
    "    client.delete_collection(collection_name)\n",
    "collection = client.create_collection(name=collection_name)\n",
    "\n",
    "# Add documents + embeddings to Chroma\n",
    "ids = [str(i) for i in range(len(doc_texts))]  # simple string IDs\n",
    "metadatas = [{\"source\": \"candidate_doc\"} for _ in doc_texts]  # optional metadata\n",
    "\n",
    "collection.add(\n",
    "    documents=doc_texts,\n",
    "    embeddings=cand_embeddings.tolist(),  # Chroma expects list of lists\n",
    "    ids=ids,\n",
    "    metadatas=metadatas\n",
    ")\n",
    "\n",
    "print(f\"Chroma collection '{collection_name}' created with {len(doc_texts)} documents.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c402434",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "11b33630",
   "metadata": {},
   "source": [
    "# Single-question retrieval demo using Chroma:\n",
    "# 1. Select a question and its gold documents.\n",
    "# 2. Embed the question using the same embedding model.\n",
    "# 3. Query the Chroma collection for top-k most similar documents.\n",
    "# 4. Display retrieved documents with a check for gold matches.\n",
    "# 5. Compute retrieval metrics: Precision@k, Recall@k, and Reciprocal Rank (RR).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c9d5826",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Who was President when the first Peanuts cartoon was published?\n",
      "Gold doc snippet: Peanuts is a syndicated daily and Sunday American comic strip written and illustrated by Charles M. Schulz, which ran from October 2, 1950, to February 13, 2000, continuing in reruns afterward. The strip is the most popular and influential in the history of comic strips, with 17,897 strips published ...\n",
      "\n",
      "Top-k retrieved docs:\n",
      "Rank 1 - Doc ID 5 ❌:\n",
      "A Brief History of Charles Schulz's 'Peanuts' Comic Strip - TIME Follow @TIME When Alex Davis was 2 years old, he pointed to a drawing his father had done and exclaimed, \"Snoopy!\" The problem: his father was Jim Davis, the creator of Garfield, and the picture was of the cat he made famous. Charles Schulz's black-and-white dog is so beloved, though, that a lasagna-loving cat can't even compete. Saturday, Oct. 2, marks 60 years since Schulz's first Peanuts strip hit newspapers. Since then, Snoopy,\n",
      "\n",
      "Rank 2 - Doc ID 1 ❌:\n",
      "Peanuts | Peanuts Wiki | Fandom powered by Wikia Charles M. Schulz drawing Snoopy . Peanuts is a syndicated daily and Sunday comic strip written and illustrated by Charles M. Schulz , which ran from October 2, 1950, to February 13, 2000 (the day after Schulz's death). In total 17,897 different Peanuts strips were published. The strip was one of the most popular and influential in the history of the medium, and considered the most beloved comic strips of all time. It was \"arguably the longest sto\n",
      "\n",
      "Rank 3 - Doc ID 4 ❌:\n",
      "Peanuts by Charles Schulz  | Read Comic Strips at GoComics.com Share Link Explore Peanuts\n",
      "\n",
      "Rank 4 - Doc ID 0 ✅ GOLD:\n",
      "Peanuts is a syndicated daily and Sunday American comic strip written and illustrated by Charles M. Schulz, which ran from October 2, 1950, to February 13, 2000, continuing in reruns afterward. The strip is the most popular and influential in the history of comic strips, with 17,897 strips published in all, making it \"arguably the longest story ever told by one human being\".  At its peak, Peanuts ran in over 2,600 newspapers, with a readership of 355 million in 75 countries, and was translated i\n",
      "\n",
      "Rank 5 - Doc ID 3 ❌:\n",
      "Frequently Asked Questions - Charles M. Schulz Museum Charles M. Schulz Museum The Peanuts Comic Strip Why is the comic strip named Peanuts? Originally, Charles Schulz named his strip Li’l Folks, but when it became syndicated in 1950 by United Feature Syndicate, there was concern about possible copyright infringement with a cartoon called Little Folks by Tack Knight that had been published in the 1930s. Schulz suggested Charlie Brown or Good Ol’ Charlie Brown, but the syndicate decided upon Pean\n",
      "\n",
      "Metrics after retrieval:\n",
      "Precision@5: 0.200\n",
      "Recall@5: 1.000\n",
      "Reciprocal Rank (RR): 0.250\n"
     ]
    }
   ],
   "source": [
    "# ----------------- Single Question Retrieval Demo -----------------\n",
    "sample = ds[0]\n",
    "question = sample[\"question\"]\n",
    "gold_docs = set(sample[\"entity_pages\"][\"wiki_context\"])  # ground truth docs\n",
    "\n",
    "print(\"Question:\", question)\n",
    "print(\"Gold doc snippet:\", list(gold_docs)[0][:300], \"...\")\n",
    "\n",
    "# Embed question\n",
    "query_emb = model.encode(question).tolist()\n",
    "\n",
    "# Retrieve top-k from Chroma\n",
    "top_k = 5\n",
    "results = collection.query(\n",
    "    query_embeddings=[query_emb],\n",
    "    n_results=top_k\n",
    ")\n",
    "\n",
    "retrieved_docs = results['documents'][0]\n",
    "retrieved_ids = results['ids'][0]  # get document IDs from Chroma\n",
    "\n",
    "# Print ranked docs with gold check\n",
    "print(\"\\nTop-k retrieved docs:\")\n",
    "for rank, (doc_id, doc) in enumerate(zip(retrieved_ids, retrieved_docs), start=1):\n",
    "    check = \"✅ GOLD\" if doc in gold_docs else \"❌\"\n",
    "    snippet = doc[:500].replace(\"\\n\", \" \")  # first 500 chars, remove line breaks\n",
    "    print(f\"Rank {rank} - Doc ID {doc_id} {check}:\\n{snippet}\\n\")\n",
    "\n",
    "# Compute metrics\n",
    "retrieved_set = set(retrieved_docs)\n",
    "precision = len(gold_docs & retrieved_set) / top_k\n",
    "recall = len(gold_docs & retrieved_set) / max(1, len(gold_docs))\n",
    "\n",
    "rr = 0\n",
    "for rank, doc in enumerate(retrieved_docs):\n",
    "    if doc in gold_docs:\n",
    "        rr = 1 / (rank + 1)\n",
    "        break\n",
    "\n",
    "print(\"Metrics after retrieval:\")\n",
    "print(f\"Precision@{top_k}: {precision:.3f}\")\n",
    "print(f\"Recall@{top_k}: {recall:.3f}\")\n",
    "print(f\"Reciprocal Rank (RR): {rr:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2556c92",
   "metadata": {},
   "source": [
    "# Single-question retrieval + reranking using Chroma and a Cross-Encoder:\n",
    "# 1. Select a question and its gold documents.\n",
    "# 2. Embed the question and retrieve top-k documents from Chroma.\n",
    "# 3. Use a cross-encoder to score (question, document) pairs for relevance.\n",
    "# 4. Rerank documents by descending score.\n",
    "# 5. Display reranked documents with gold match check.\n",
    "# 6. Compute retrieval metrics (Precision@k, Recall@k, Reciprocal Rank) after reranking.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c81ff04d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Who was President when the first Peanuts cartoon was published?\n",
      "Gold doc snippet: Peanuts is a syndicated daily and Sunday American comic strip written and illustrated by Charles M. Schulz, which ran from October 2, 1950, to February 13, 2000, continuing in reruns afterward. The strip is the most popular and influential in the history of comic strips, with 17,897 strips published ...\n",
      "\n",
      "Reranked top-k docs:\n",
      "Rank 1 - Doc ID 1 ❌:\n",
      "Peanuts | Peanuts Wiki | Fandom powered by Wikia Charles M. Schulz drawing Snoopy . Peanuts is a syndicated daily and Sunday comic strip written and illustrated by Charles M. Schulz , which ran from October 2, 1950, to February 13, 2000 (the day after Schulz's death). In total 17,897 different Peanuts strips were published. The strip was one of the most popular and influential in the history of the medium, and considered the most beloved comic strips of all time. It was \"arguably the longest sto\n",
      "\n",
      "Rank 2 - Doc ID 0 ✅ GOLD:\n",
      "Peanuts is a syndicated daily and Sunday American comic strip written and illustrated by Charles M. Schulz, which ran from October 2, 1950, to February 13, 2000, continuing in reruns afterward. The strip is the most popular and influential in the history of comic strips, with 17,897 strips published in all, making it \"arguably the longest story ever told by one human being\".  At its peak, Peanuts ran in over 2,600 newspapers, with a readership of 355 million in 75 countries, and was translated i\n",
      "\n",
      "Rank 3 - Doc ID 5 ❌:\n",
      "A Brief History of Charles Schulz's 'Peanuts' Comic Strip - TIME Follow @TIME When Alex Davis was 2 years old, he pointed to a drawing his father had done and exclaimed, \"Snoopy!\" The problem: his father was Jim Davis, the creator of Garfield, and the picture was of the cat he made famous. Charles Schulz's black-and-white dog is so beloved, though, that a lasagna-loving cat can't even compete. Saturday, Oct. 2, marks 60 years since Schulz's first Peanuts strip hit newspapers. Since then, Snoopy,\n",
      "\n",
      "Rank 4 - Doc ID 3 ❌:\n",
      "Frequently Asked Questions - Charles M. Schulz Museum Charles M. Schulz Museum The Peanuts Comic Strip Why is the comic strip named Peanuts? Originally, Charles Schulz named his strip Li’l Folks, but when it became syndicated in 1950 by United Feature Syndicate, there was concern about possible copyright infringement with a cartoon called Little Folks by Tack Knight that had been published in the 1930s. Schulz suggested Charlie Brown or Good Ol’ Charlie Brown, but the syndicate decided upon Pean\n",
      "\n",
      "Rank 5 - Doc ID 4 ❌:\n",
      "Peanuts by Charles Schulz  | Read Comic Strips at GoComics.com Share Link Explore Peanuts\n",
      "\n",
      "Metrics after reranking:\n",
      "Precision@5: 0.200\n",
      "Recall@5: 1.000\n",
      "Reciprocal Rank (RR): 0.500\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import CrossEncoder\n",
    "\n",
    "# ----------------- Single Question Retrieval + Rerank -----------------\n",
    "sample = ds[0]\n",
    "question = sample[\"question\"]\n",
    "gold_docs = set(sample[\"entity_pages\"][\"wiki_context\"])  # ground truth docs\n",
    "\n",
    "print(\"Question:\", question)\n",
    "print(\"Gold doc snippet:\", list(gold_docs)[0][:300], \"...\")\n",
    "\n",
    "# Embed question\n",
    "query_emb = model.encode(question).tolist()\n",
    "\n",
    "# Retrieve top-k from Chroma\n",
    "top_k = 5\n",
    "results = collection.query(\n",
    "    query_embeddings=[query_emb],\n",
    "    n_results=top_k\n",
    ")\n",
    "\n",
    "retrieved_docs = results['documents'][0]\n",
    "retrieved_ids = results['ids'][0]  # doc IDs\n",
    "\n",
    "# ----------------- Rerank with Cross-Encoder -----------------\n",
    "cross_model = CrossEncoder(\"cross-encoder/ms-marco-MiniLM-L-6-v2\")\n",
    "pairs = [[question, doc] for doc in retrieved_docs]\n",
    "scores = cross_model.predict(pairs)\n",
    "\n",
    "# Sort by scores descending\n",
    "reranked = sorted(zip(scores, retrieved_ids, retrieved_docs), reverse=True)\n",
    "reranked_scores, reranked_ids, reranked_docs = zip(*reranked)\n",
    "\n",
    "# Print reranked top-k\n",
    "print(\"\\nReranked top-k docs:\")\n",
    "for rank, (doc_id, doc) in enumerate(zip(reranked_ids, reranked_docs), start=1):\n",
    "    check = \"✅ GOLD\" if doc in gold_docs else \"❌\"\n",
    "    snippet = doc[:500].replace(\"\\n\", \" \")\n",
    "    print(f\"Rank {rank} - Doc ID {doc_id} {check}:\\n{snippet}\\n\")\n",
    "\n",
    "# ----------------- Compute Metrics -----------------\n",
    "retrieved_set = set(reranked_docs)\n",
    "precision = len(gold_docs & retrieved_set) / top_k\n",
    "recall = len(gold_docs & retrieved_set) / max(1, len(gold_docs))\n",
    "\n",
    "rr = 0\n",
    "for rank, doc in enumerate(reranked_docs):\n",
    "    if doc in gold_docs:\n",
    "        rr = 1 / (rank + 1)\n",
    "        break\n",
    "\n",
    "print(\"Metrics after reranking:\")\n",
    "print(f\"Precision@{top_k}: {precision:.3f}\")\n",
    "print(f\"Recall@{top_k}: {recall:.3f}\")\n",
    "print(f\"Reciprocal Rank (RR): {rr:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "769c79d1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".conda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
